# MCP Server Configuration
MCP_SERVER_PORT=8000
MCP_SERVER_HOST=127.0.0.1
NODE_ENV=development

# Authentication Configuration (optional)
# Comma-separated list of API keys for bearer token authentication
# Leave empty to disable authentication
# AUTH_BEARER_KEYS=key1,key2,key3

# GitLab Configuration
GITLAB_TOKEN=glpat-xxxxxxxxxxxxxxxxxxxx
GITLAB_API=https://gitlab.com/api/v4
GITLAB_DEFAULT_PROJECT=group/subgroup/project

# GitHub Configuration
GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx
GITHUB_DEFAULT_REPO=owner/repo

# LLM Configuration
# Supported providers: openrouter, openai, ollama, azure
LLM_PROVIDER=openrouter
LLM_API_KEY=sk-or-v1-xxxxxxxxxxxxxxxxxxxx
LLM_MODEL=openai/gpt-oss-120b:exacto

# Optional: Custom LLM API URL (for Azure, custom endpoints, etc.)
# LLM_API_URL=

# General Configuration
MAX_DIFF_CHARS=50000

# Examples for different LLM providers:

# OpenRouter (default):
#   LLM_PROVIDER=openrouter
#   LLM_API_KEY=sk-or-v1-...
#   LLM_MODEL=openai/gpt-oss-120b:exacto

# OpenAI:
#   LLM_PROVIDER=openai
#   LLM_API_KEY=sk-...
#   LLM_MODEL=gpt-4o

# Ollama (local, no API key needed):
#   LLM_PROVIDER=ollama
#   LLM_MODEL=llama3.1:8b
#   LLM_API_URL=http://localhost:11434/v1/chat/completions

# Azure OpenAI:
#   LLM_PROVIDER=azure
#   LLM_API_KEY=your_azure_key
#   LLM_MODEL=gpt-4
#   LLM_API_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment/chat/completions?api-version=2024-02-15-preview

# Proxy Mode Configuration
# Enable proxy mode for the mcp-server
# PROXY_MODE=1
# Comma-separated list of valid slave authentication codes
# SLAVE_CODES=code1,code2,code3
# WebSocket port for slave connections (optional, default: 8001)
# PROXY_WS_PORT=8001

# Slave Instance Configuration
# Enable slave mode for the mcp-server
# PROXY_SLAVE=1
# Authentication code (must be in proxy's SLAVE_CODES)
# SLAVE_CODE=code1
# WebSocket URL of the proxy server
# PROXY_SERVER_URL=ws://localhost:8001
# HTTP port for this slave (standard config, e.g., 9000)
# MCP_SERVER_PORT=9000
